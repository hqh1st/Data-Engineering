import os
import re
import fitz
import faiss
import pickle
import numpy as np
import networkx as nx
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import matplotlib.pyplot as plt
import jieba.analyse
import networkx as nx
from itertools import combinations
from collections import Counter

PDF_FOLDER = "data"  # ä½ çš„å¤šPDFç›®å½•
INDEX_PATH = "bd/index.faiss"
SENTENCES_PATH = "bd/sentences_with_meta.pkl"

client = OpenAI(
    api_key="sk-14abb7efe7b541d4b7e3e871460b583b",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# === æå–å•ä¸ª PDF çš„å¥å­ï¼ˆå¸¦æ–‡ä»¶åå’Œé¡µç ï¼‰===
def extract_sentences_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    filename = os.path.basename(pdf_path)
    result = []

    for page_num, page in enumerate(doc):
        text = page.get_text("text")
        lines = text.split("\n")

        clean_lines = []
        for line in lines:
            line = line.strip()

            # === âœ‚ï¸ å¿½ç•¥ä»¥ä¸‹å™ªå£°ç±»å†…å®¹ ===
            if not line or len(line) < 4:
                continue  # å¤ªçŸ­è·³è¿‡
            if re.match(r"^\d{1,3}$", line):  # é¡µç 
                continue
            if any(keyword in line for keyword in [
                "è”ç³»æˆ‘ä»¬", "æœåŠ¡å·", "æ‰«ç å…³æ³¨", "æ‰«æäºŒç»´ç ", "ç‰ˆæƒæ‰€æœ‰", "Thoughtworks", "æ´è§",
                "ç›®å½•", "ç›®å½•é¡µ", "å¾®ä¿¡", "é‚®ç®±", "ç”µè¯", "å…¬ä¼—å·"
            ]):
                continue
            clean_lines.append(line)

        merged_text = "".join(clean_lines)

        # === æŒ‰ä¸­è‹±æ–‡æ ‡ç‚¹åˆ†å¥ ===
        sentence_endings = r'([ã€‚ï¼ï¼Ÿ?!.])'
        parts = re.split(sentence_endings, merged_text)
        sentences = [parts[i] + parts[i+1] if i+1 < len(parts) else parts[i]
                     for i in range(0, len(parts), 2)]

        for sent in sentences:
            sent = sent.strip()
            if len(sent) > 5:
                result.append((sent, page_num + 1, filename))
    return result

# === æ‰¹é‡æå–æ‰€æœ‰ PDF ===
def extract_all_pdfs(folder):
    all_sentences = []
    for fname in os.listdir(folder):
        if fname.endswith(".pdf"):
            print(f"ğŸ“„ æ­£åœ¨å¤„ç†: {fname}")
            full_path = os.path.join(folder, fname)
            all_sentences.extend(extract_sentences_from_pdf(full_path))
    return all_sentences

# === æ„å»ºç»Ÿä¸€å‘é‡æ•°æ®åº“ ===
def build_faiss_index(sentences_with_meta):
    sentences = [s for s, _, _ in sentences_with_meta]
    model = SentenceTransformer("all-MiniLM-L6-v2")
    vectors = model.encode(sentences, show_progress_bar=True)
    vectors = np.array(vectors).astype("float32")
    faiss.normalize_L2(vectors)

    dim = vectors.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(vectors)

    os.makedirs(os.path.dirname(INDEX_PATH), exist_ok=True)
    faiss.write_index(index, INDEX_PATH)
    with open(SENTENCES_PATH, "wb") as f:
        pickle.dump(sentences_with_meta, f)

    return index, sentences_with_meta

# === è½½å…¥ç´¢å¼•å’Œå¥å­ä¿¡æ¯ ===
def load_index():
    print("ğŸ“‚ æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“...")
    index = faiss.read_index(INDEX_PATH)
    with open(SENTENCES_PATH, "rb") as f:
        meta = pickle.load(f)
    return index, meta

# === æ„å»ºå›¾ï¼ˆå¯é€‰ï¼‰===
def build_sentence_graph(sentences_with_meta, window=1):
    G = nx.DiGraph()
    for i, (sent, page, source) in enumerate(sentences_with_meta):
        G.add_node(i, text=sent, page=page, source=source)
        for j in range(1, window + 1):
            if i + j < len(sentences_with_meta):
                G.add_edge(i, i + j)
    return G

# === æ£€ç´¢ç›¸ä¼¼å¥ç´¢å¼• + æ‰©å±•ä¸Šä¸‹æ–‡ ===
def search_and_expand(query, index, meta, graph=None, top_k=5, expand_k=1):
    model = SentenceTransformer("all-MiniLM-L6-v2")
    q_vec = model.encode([query]).astype("float32")
    faiss.normalize_L2(q_vec)
    _, indices = index.search(q_vec, top_k)

    matched = indices[0]
    if graph:
        expanded = set(matched)
        for idx in matched:
            expanded.update(list(graph.successors(idx))[:expand_k])
            expanded.update(list(graph.predecessors(idx))[:expand_k])
        return sorted(expanded)
    else:
        return matched

# === æ¨¡å‹å›ç­”ç”Ÿæˆ ===
def generate_answer(query, context):
    if not context.strip():
        prompt = "æ— å¯ç”¨æ•°æ®ï¼Œç¦æ­¢ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†ï¼Œè¯·å›ç­”â€œæ— æ³•å›ç­”â€ã€‚"
    else:
        prompt = (
            "ä»¥ä¸‹æ˜¯å¤šä¸ªPDFæ–‡æ¡£ä¸­çš„ç›¸å…³ç‰‡æ®µï¼Œè¯·ä»…åŸºäºè¿™äº›å†…å®¹ä½œç­”ï¼Œ"
            "ä¸å¾—ä½¿ç”¨å¸¸è¯†æˆ–é¢„è®­ç»ƒçŸ¥è¯†ï¼š\n" + context
        )
    completion = client.chat.completions.create(
        model="qwen-plus",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£é—®ç­”åŠ©æ‰‹ï¼Œåªèƒ½åŸºäºä¸Šä¸‹æ–‡å†…å®¹ä½œç­”ã€‚"},
            {"role": "user", "content": f"{prompt}\n\né—®é¢˜ï¼š{query}"}
        ]
    )
    return completion.choices[0].message.content

# === çŸ¥è¯†é—®ç­”å…¥å£ ===
def knowledge_qa(query, index, meta, graph=None):
    print("\nğŸ” æ­£åœ¨æ£€ç´¢...")
    match_ids = search_and_expand(query, index, meta, graph)
    if not match_ids:
        print("âŒ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
        context = ""
    else:
        context_lines = []
        print("âœ… æ£€ç´¢åˆ°ä»¥ä¸‹ç‰‡æ®µï¼š")
        for idx in match_ids:
            sent, page, source = meta[idx]
            print(f"[{source} - ç¬¬{page}é¡µ]: {sent}")
            context_lines.append(f"[{source} ç¬¬{page}é¡µ] {sent}")
        context = "\n".join(context_lines)
    answer = generate_answer(query, context)
    print("\nğŸ’¬ å›ç­”ï¼š", answer)
    
def visualize_sentence_graph(graph, save_path="bd/sentence_graph.png"):
    if not os.path.exists(os.path.dirname(save_path)):
        os.makedirs(os.path.dirname(save_path))

    plt.figure(figsize=(18, 18))  # å¯ä»¥è°ƒæ•´å°ºå¯¸
    pos = nx.spring_layout(graph, k=0.2)  # è‡ªåŠ¨å¸ƒå±€
    nx.draw_networkx_nodes(graph, pos, node_size=100, node_color='skyblue')
    nx.draw_networkx_edges(graph, pos, arrows=True, arrowstyle='->', width=1.0)
    nx.draw_networkx_labels(graph, pos, font_size=6, font_color='black')

    plt.axis("off")
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    print(f"ğŸ“ å¥é—´è¿æ¥å›¾å·²ä¿å­˜è‡³: {save_path}")
    plt.close()



# === ä¸»å‡½æ•° ===
def main():
    if not os.path.exists(INDEX_PATH) or not os.path.exists(SENTENCES_PATH):
        print("ğŸš§ åˆæ¬¡è¿è¡Œï¼šä»å¤šä¸ªPDFä¸­æ„å»ºå‘é‡æ•°æ®åº“...")
        all_data = extract_all_pdfs(PDF_FOLDER)
        index, meta = build_faiss_index(all_data)
    else:
        index, meta = load_index()

    graph = build_sentence_graph(meta)
    
    visualize_sentence_graph(graph)
    
    while True:
        query = input("\nâ“ è¯·è¾“å…¥é—®é¢˜ï¼ˆæˆ–è¾“å…¥ 'é€€å‡º' ç»“æŸï¼‰ï¼š")
        if query.strip().lower() == "é€€å‡º":
            print("ğŸ‘‹ é€€å‡ºé—®ç­”ç³»ç»Ÿ")
            break
        knowledge_qa(query, index, meta, graph)

# === å¯åŠ¨ ===
if __name__ == "__main__":
    main()